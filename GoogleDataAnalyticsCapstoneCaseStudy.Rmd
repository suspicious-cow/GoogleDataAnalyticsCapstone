---
title: "Case Study: How Does a Bike-Share Navigate Speedy Success?"
author: "Zain Naboulsi"
date: "2023-03-22"
bibliography: references.bib
output:
  html_document:
    toc: yes
    
  pdf_document:
    toc: yes
    
editor_options: 
  markdown: 
    wrap: 80
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE}
# set a seed in case we use any random numbers
set.seed(1337)


# Set the names of the packages and libraries you want to install
required_libraries <- c("tidyverse", "purrr", "knitr", "skimr", "readr")

# Install missing packages and load all required libraries
for (lib in required_libraries) {
  if (!requireNamespace(lib, quietly = TRUE)) {
    install.packages(lib)
  }
  library(lib, character.only = TRUE)
}

# load all the cycling data for analysis
# our data runs from 04/2022 to 04/2023
if(file.exists("combined_data_df.rds")) {
    # read in our combined data
    combined_data_df <- readRDS("combined_data_df.rds")
    
} else {
    
    # get file list
    file_list <- list.files(path = "SourceData/", pattern = "*.csv", full.names = TRUE)
    
    # read and combine files
    combined_data_df <- map_df(file_list, read_csv)
    
    # convert ridable_type and member_casual into factors
    combined_data_df$rideable_type <- as.factor(combined_data_df$rideable_type)
    combined_data_df$member_casual <- as.factor(combined_data_df$member_casual)
    
    # save the combined_data object so we don't have to reload this data again
    # unless it changes
    write_rds(combined_data_df, "combined_data_df.rds")
}

```

# Abstract
Cyclistic, a popular bike-share company based in Chicago, strives to maximize 
its annual memberships for sustainable growth. The company has an existing user 
base composed of both casual riders (who opt for single-ride or full-day passes) 
and annual members. The focus of this case study is to understand the 
utilization patterns of these two user groups, as the company believes that 
converting casual riders into annual members can significantly enhance their 
profitability. To devise effective marketing strategies for this conversion, the 
case study will delve into questions that probe how these two user groups use 
Cyclistic bikes differently, why casual riders might opt for annual memberships, 
and how digital media can facilitate this conversion.

This study is divided into six stages: Ask, Prepare, Process, Analyze, Share, 
and Act. Each section will come with additional detail and build on the previous 
section. We hope you enjoy reading this analysis as much as we enjoyed making
it. All work was done with the R Language [@rbase] in R Studio [@rstudio].


# Ask
The primary business question we aim to address is: How do annual members and 
casual riders use Cyclistic bikes differently? This question is fundamental to 
developing a marketing strategy that can effectively convert casual riders into 
annual members.

To answer this question, we will delve into the following topics:

1. *Duration and frequency of rides:* We will analyze data to understand the 
typical length and frequency of rides for both user groups. Are there noticeable 
differences between casual riders and annual members? Are members using the 
bikes more frequently or for longer duration?

2. *Purpose of rides:* We aim to determine the typical purpose of rides for both 
groups. Do casual riders primarily use the bikes for leisure, while members use 
them more for daily commuting? Or is there an overlap?

3. *Ride timings:* We'll investigate if there's a difference in the time of day, 
day of the week, or month of the year when the two user groups typically use the 
bikes.

4. *Bike preferences:* Given that Cyclistic offers various bike options, we will 
examine if there's a pattern in the type of bike preferred by the two user 
groups.

Answering these questions will provide insights into the riding patterns of the 
two groups, and will be instrumental in forming the foundation for a marketing 
strategy aimed at converting casual riders into annual members. The strategy 
will be developed in alignment with these insights, addressing the unique needs 
and behaviors of casual riders to motivate their transition to becoming annual 
members.

Cyclistic's collected data forms the backbone of this analysis. For 
transparency, we have included the source code in the paper's Appendix. We 
strive to communicate our findings clearly, pairing concise language with 
comprehensible visualizations and straightforward recommendations.

# Prepare  

The data for this analysis is publicly available and located on Divvy's Amazon 
S3 storage. The data is titled as "Divvy Data." [@DivvyData2023]

The data is organized in an anonymized format, detailing each trip's start and 
end day/time, start and end stations, and rider type (Member or Casual). Trips 
taken by staff and any trips under 60 seconds in length have been excluded from 
this dataset.

Given the anonymization and rigorous data handling practices outlined by Divvy, 
we can consider the data to be Reliable, Original, Comprehensive, Current, and 
Cited (ROCCC). However, some level of bias could be inherent as the data only 
includes riders who choose to use Divvy's services, and not all potential or 
actual cyclists in Chicago. Also, the credibility of the dataset is tied to 
Divvy's data collection and handling processes.

The data is provided according to the Divvy Data License Agreement. 
Accessibility is facilitated through public download links, ensuring the data 
can be freely used for analysis while respecting privacy through anonymization. 
Security of the data in our analysis will be ensured by following best 
practices, such as secure data storage and handling.

Data integrity has been verified through initial exploratory data analysis to 
check for missing values, inconsistencies, or obvious outliers. Any issues found 
during this process will be documented and addressed appropriately.

This dataset is instrumental in answering our primary business question â€” 
understanding how annual members and casual riders use Cyclistic bikes 
differently. Each trip's information, coupled with the type of rider, will offer 
insights into the usage patterns of the two user groups.

Potential problems with the data could include missing values, inconsistent data 
entries, or anomalies. These will be explored and addressed during the data 
cleaning and preprocessing stages of our analysis. If severe issues are 
encountered, they will be documented and taken into consideration when 
interpreting the analysis results.

## Structure of the Data
We used the str() function in R to give us a detailed structure of the data.  

```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE}

# examine the structure of the data
str(combined_data_df)

```

Here are some interesting observations from the output:  

* __Data size:__ The dataframe contains over 6.23 million records, spread across 13 
variables.  

* __Data types:__ The variables in the dataframe are of various types, including 
character (e.g., ride_id, start_station_name, start_station_id, etc.), 
factor (e.g., rideable_type, member_casual), numeric (e.g., start_lat, 
start_lng, end_lat, end_lng), and POSIXct (e.g., started_at, ended_at). This 
variety indicates that the dataset is rich and contains different kinds of 
information.  

* __Rideable types:__ There are three levels of rideable_type: classic_bike, 
docked_bike, and electric_bike.  

* __Member categories:__ There are two levels of member_casual: casual and member.  

* __Geographic coordinates:__ The data contains latitude and longitude information 
for both the start and end locations of rides. This information could be used to 
plot geographic routes or to analyze usage patterns across different locations.  

* __Time information:__ The dataset includes when each ride started and ended. This 
information can be used to analyze the temporal patterns of bike usage.

* __Station information:__ The data includes detailed information about the start 
and end stations, including their names and IDs.

## Duplicate Observations
Now we turn our attention to broad issues in the data if they exist. The first 
task we will undertake is to check for duplicate rows in our data.  

```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE}

# Identify duplicates
duplicates <- duplicated(combined_data_df)

# Count duplicates
num_duplicates <- sum(duplicates)

# Show the number of duplicates
cat("Total number of duplicate rows found:",num_duplicates)

```  

Fortunately, this dataset doesn't have any duplicate data. We can now move on to 
other metrics.  

## Missing Values
Our next task is to look for missing data (or NA data, as it is commonly 
called). Here is what we found:

```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE}

# Check for missing values
colSums(is.na(combined_data_df))

```

From this output, we can see that there are missing values (NA) in some of the 
columns.

- The `start_station_name` column has 902,896 missing values.
- The `start_station_id` column has 903,028 missing values.
- The `end_station_name` column has 964,949 missing values.
- The `end_station_id` column has 965,090 missing values.
- The `end_lat` and `end_lng` columns each have 6,290 missing values.

The other columns (`ride_id`, `rideable_type`, `started_at`, `ended_at`, 
`start_lat`, `start_lng`, and `member_casual`) do not have any missing values.

Missing data can be problematic because it may skew the results of our analysis 
or cause certain functions to fail. You will want to investigate why there are 
missing values in the data and consider ways of handling it, such as filling 
them in with an average or other statistic (imputation), or removing rows. 

The differences in the number of missing values between the `start` and `end` 
station names and IDs could indicate inconsistencies in data collection or 
recording. This may require further investigation to ensure that any analyses 
we conduct on this data are valid.


## Summary Statistics
Now we an run some summary statistics to get an overall view of the data 
makeup and how we might approach our analysis. 

```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE}

# Print detailed summary statistics of the dataframe
skim(combined_data_df)

```

Here are some insights we can glean:

* __Data dimensions:__ The dataset contains 6,230,310 rows (observations) and 
13 columns (variables). It's a large dataset.

* __Variable types:__ There are five character variables, two factors, 
four numeric variables, and two POSIXct (date-time) variables. This mix of types 
suggests a varied dataset, likely to contain both categorical and continuous 
data, as well as time series data.

* __Missing Values:__ The start and end station names and IDs have around 15% 
missing values, given that the complete rate is around 0.85. This could be a 
potential area of concern depending on how we want to use this data.

* __Unique Values:__ The ride_id variable is unique for every row, which 
suggests that it might be a unique identifier for each ride.

* __Rideable Type:__ There are three types of bikes used in these rides: 
electric bikes, classic bikes, and docked bikes. The most common type of bike 
used is the electric bike, followed by the classic bike.

* __Membership:__ There are more rides by members (3,745,586) than by casual 
users (2,484,724).

* __Locations:__ The latitude and longitude for start and end points of the 
rides are quite concentrated around certain values (41.9 for latitude and -87.6 
for longitude), suggesting that most rides occur in a relatively confined 
geographical area.

* __Dates:__ The started_at and ended_at fields indicate that the data spans 
from 1st April 2022 to 30th April 2023, so just over a year's worth of ride 
data.


# Process
Turning our attention to processing the data, we can review what decisions we 
have made thus far and augment the data as needed to facilitate the analysis 
process coming up. 

## Key Decisions
Here are the decisions made so far:
* __Tools chosen__ include R programming language along with the `tidyverse`, `purrr`, 
`knitr`, `skimr`, and `readr` libraries. The reason for choosing these tools is 
their strength in data manipulation, visualization, and analysis. `tidyverse` 
includes several useful packages for data cleaning and analysis, while `purrr` 
enhances R's functional programming capabilities. `knitr` provides dynamic 
report generation, and `skimr` and `readr` are helpful for data summarization 
and importing.

* __Data's integrity__ is ensured through multiple steps: Firstly, by loading 
the dataset directly from a local ".rds" file if it exists to avoid data 
corruption that could occur during multiple read operations. If the ".rds" file 
does not exist, CSV files are read directly from the source directory, ensuring 
the original data's integrity.

* __Data cleaning__ steps taken involve checking for missing values and 
duplicated records. The number of missing values in each column is found using 
`colSums(is.na(combined_data_df))`, and duplicated records are found using 
`duplicated(combined_data_df)`.

* __Data verification__ for readiness for analysis is done by summarizing the 
dataframe using `skim(combined_data_df)` which provides detailed summary 
statistics. This summary provides a good understanding of the data's 
distribution and potential issues (like extreme outliers).

* __Cleaning processes__ are documented in this markdown document. Including, 
checking for missing values, identifying duplicates, and getting summary 
statistics. This document serves as a record of the cleaning process and can be 
shared or reviewed as needed.

## Additional Data Changes  
Beyond the standard processing steps to get the data ready for analysis, we 
decided to make a couple of modifications to the dataframe to support additional 
queries we anticipate. The changes are as follows:  

* __Calculate Trip Time:__ The `ride_length` variable was added. This column is a 
derived by subtracting `ended_at` from `started_from`. The results are in total 
minutes elapsed for the ride.  

* __Start Date:__ One new column (`started_date`) was added to strip out the 
time from the original `started_at` column. This allows for plots to examine 
rental trends over time by day. 

* __Day of Week:__ A two new columns entitled `day_of_week_start` and 
`day_of_week_end` were added to make analysis of the specific day (e.g. Monday, 
Tuesday, etc...) a trip happened easier. This is converted to a factor for more 
advanced activities.  

```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE}

# calculate the total ride time in minutes
combined_data_df$ride_length <- 
    round((combined_data_df$ended_at - combined_data_df$started_at) / 60, 2)

# Change the units attribute to "minutes"
attr(combined_data_df$ride_length, "units") <- "minutes"

# Add a new column for the date without the time
combined_data_df$started_date <- as.Date(combined_data_df$started_at)

# Create the 'day_of_week_start' and 'day_of_week_end' columns
combined_data_df$day_of_week_start <- wday(combined_data_df$started_at, label = TRUE)
combined_data_df$day_of_week_end <- wday(combined_data_df$ended_at, label = TRUE)

# Convert the new columns to factors
combined_data_df$day_of_week_start <- as.factor(combined_data_df$day_of_week_start)
combined_data_df$day_of_week_end <- as.factor(combined_data_df$day_of_week_end)


```

## Removing Outliers
After adding the ride_length variable we discovered some outliers in the data. 
Specifically, we found start and end trip times that would result in a negative 
time and we found values that were well beyond the two day "normal" time for a 
ride. In total there were 1,346 of these rows. We opted to remove them for this 
dataset.

```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE}

# Get rid of outliers by only accepting ride times that are less than two days 
# and greater than zero
combined_data_df <- combined_data_df %>% filter(ride_length <= 2880 & ride_length > 0) 


```


# Analyze

## Rental Basic Metrics 
To get started, we wanted to present some basic metrics on ride length:  

```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE}
# ride_length metrics
ride_length_mean <- mean(as.numeric(combined_data_df$ride_length))
ride_length_max <- max(as.numeric(combined_data_df$ride_length))
ride_length_min <- min(as.numeric(combined_data_df$ride_length))

cat("The average rental time was", round(ride_length_mean, 2), 
    "minutes long.\n The longest rental time was", ride_length_max,"minutes\n",
    "and the shortest rental time was", ride_length_min, "minutes.")


```

As we can see, the average rental is about seventeen minutes but the range of 
rental times varies widely from a few seconds to two days. Because we removed 
outliers we may have removed legitimate rentals over two days; however they 
would be anomalies and not indicative of the overall data. 

## Daily Bike Rentals
It is important to take a step back and look at the entire year of data we have 
to get some perspective on rental trends. The graph below shows the daily bike 
rentals from April 2022 to April 2023. 

```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE}

# Group by date and count the number of rides each day
daily_counts <- combined_data_df %>% group_by(started_date) %>% summarise(count = n())

# Line graph for the year with trendline
ggplot(daily_counts, aes(x = started_date, y = count)) +
  geom_line(color = "darkblue") +
  geom_smooth(method = "loess", se = FALSE, color = "red", linetype="dashed") +
  labs(x = "Date", 
       y = "Number of Rentals", 
       title = "Daily Bike Rentals", 
       subtitle = "With trend line",
       caption = "Source: Your Data Source") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 20),
        plot.subtitle = element_text(face = "italic", size = 12),
        axis.title = element_text(face = "bold", size = 14),
        axis.text = element_text(size = 12))



```


\newpage
# Appendix

## References  

<div id="refs"></div>  

## Source Code  

```{r ref.label = knitr::all_labels(appendix == TRUE), echo=TRUE, eval=FALSE}
